{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter notebook\n",
    "Copy (fork) and edit as many copies of this notebook as you require "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-06T15:56:36.745832Z",
     "iopub.status.busy": "2025-10-06T15:56:36.745119Z",
     "iopub.status.idle": "2025-10-06T15:56:38.744392Z",
     "shell.execute_reply": "2025-10-06T15:56:38.743657Z",
     "shell.execute_reply.started": "2025-10-06T15:56:36.745764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 72)\n",
    "\n",
    "seed = 42\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 6.8)\n",
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1)\n",
    "#sns.set_style(\"whitegrid\")\n",
    "\n",
    "import plotly.io as pio\n",
    "# for use in JupyterLab 4\n",
    "pio.renderers.default = 'iframe'\n",
    "# for use in Google Colab\n",
    "#pio.renderers.default = 'colab'\n",
    "import plotly as py\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:56:38.746407Z",
     "iopub.status.busy": "2025-10-06T15:56:38.745959Z",
     "iopub.status.idle": "2025-10-06T15:56:38.841396Z",
     "shell.execute_reply": "2025-10-06T15:56:38.840545Z",
     "shell.execute_reply.started": "2025-10-06T15:56:38.746378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the test data that you are asked to make predictions for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:57:05.885591Z",
     "iopub.status.busy": "2025-10-06T15:57:05.884831Z",
     "iopub.status.idle": "2025-10-06T15:57:05.924461Z",
     "shell.execute_reply": "2025-10-06T15:57:05.923725Z",
     "shell.execute_reply.started": "2025-10-06T15:57:05.88556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural cleaning before splitting\n",
    "def structural_cleaning(df, is_train=True):\n",
    "   df = df.copy()\n",
    "\n",
    "   # A. Filter target (only train)\n",
    "   if is_train and 'DBWT' in df.columns:\n",
    "       df = df[df['DBWT'] != 9999]\n",
    "\n",
    "   # B. Null codes\n",
    "   # Map 99, 999 ...\n",
    "   cols_99 = ['CIG_0', 'FAGECOMB', 'M_Ht_In', 'PREVIS']\n",
    "   for col in cols_99:\n",
    "       if col in df.columns:\n",
    "           df[col] = df[col].replace(99, np.nan)\n",
    "\n",
    "   cols_9 = ['FEDUC', 'MEDUC', 'BFACIL', 'ATTEND']\n",
    "   for col in cols_9:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(9, np.nan)\n",
    "\n",
    "   if 'BMI' in df.columns:\n",
    "        df['BMI'] = df['BMI'].replace(99.9, np.nan)\n",
    "\n",
    "   # C. Structural nullity (888 -> 0)\n",
    "   cols_888 = ['ILLB_R', 'ILOP_R', 'ILP_R']\n",
    "   for col in cols_888:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(888, 0)\n",
    "            df[col] = df[col].replace(999, np.nan)\n",
    "\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply structural cleaning to every df\n",
    "train_cleaned = structural_cleaning(train, is_train=True)\n",
    "test_cleaned = structural_cleaning(test, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting is based into 3 conjuntos\n",
    "1. Conjunto train: Is used so the model can learn parameters\n",
    "2. Conjunto validation: To select the best model and set hyperparameters to estimate errors of generalization during development.\n",
    "3. Conjunto test: Untouchable until the end. Gives a non-biassed estimation about how the model works irl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable is 'Delivery Birth Weight' (DBWT)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate target variable to isolate the features (X)\n",
    "y = train_cleaned.pop('DBWT')\n",
    "X = train_cleaned\n",
    "\n",
    "# 80/20 ratio\n",
    "# Split the 20% of data for final testing\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# From the 80% training data, split again into training and validation sets (75/25 ratio)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape} (aprox 60%)\")\n",
    "print(f\"Val shape:   {X_val.shape}   (aprox 20%)\")\n",
    "print(f\"Test shape:  {X_test.shape}  (aprox 20%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a baseline signifies that every model turned in after this must best this result no matter what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Define the prediction (mean of training target)\n",
    "baseline_pred = y_train.mean()\n",
    "print(f\"Baseline value: {baseline_pred:.2f}\")\n",
    "\n",
    "# Create a dummy prediction vector array for validation\n",
    "y_val_pred = np.full(len(y_val), baseline_pred)\n",
    "\n",
    "# Evaluate RMSE\n",
    "rmse = root_mean_squared_error(y_val, y_val_pred)\n",
    "print(f\"Baseline RMSE on validation set: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical imputation is the process of replacing missing data points in a dataset with plausible substituted values to maintaint data integrity. I did the splitting before filling in data to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting all the info from `UserGuide2018-508` shared in the Kaggle description I made a full data cleanup to try a standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that do not give predictive value\n",
    "drop_cols = ['id', 'DLMP_MM', 'DOB_TT'] # Exact dates that give noise\n",
    "\n",
    "# Numerical columns -> Candidates for median + scaling\n",
    "# Education has order: We try as a numerical\n",
    "num_cols = [\n",
    "    'BMI', 'CIG_0', 'FAGECOMB', 'ILLB_R', 'ILOP_R', 'ILP_R', 'MAGER',\n",
    "    'M_Ht_In', 'PRECARE', 'PREVIS', 'PRIORDEAD', 'PRIORLIVE', 'PRIORTERM',\n",
    "    'PWgt_R', 'RF_CESARN', 'WTGAIN', 'FEDUC', 'MEDUC'\n",
    "]\n",
    "\n",
    "# Categorical columns -> Candidates for mode + one-hot encoding\n",
    "# Text variables that do not have order\n",
    "cat_cols = [\n",
    "    'ATTEND', 'BFACIL', 'DMAR', 'DOB_MM', 'DOB_WK', 'LD_INDL',\n",
    "    'MBSTATE_REC', 'PAY', 'PAY_REC', 'RDMETH_REC', 'RESTATUS',\n",
    "    'RF_CESAR', 'SEX', 'NO_INFEC', 'NO_MMORB', 'NO_RISKS'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training stats learning\n",
    "def learn_imputation_stats(df):\n",
    "   stats = {}\n",
    "\n",
    "   # A. Numerical -> Median\n",
    "   for col in num_cols:\n",
    "      if col in df.columns:\n",
    "         stats[col] = df[col].median()\n",
    "\n",
    "   # B. Categorical -> Mode\n",
    "   for col in cat_cols:\n",
    "      if col in df.columns:\n",
    "         # dropna=True to avoid NaN in mode calculation\n",
    "         stats[col] = df[col].mode(dropna=True)[0]\n",
    "\n",
    "   return stats\n",
    "imputation_stats = learn_imputation_stats(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Really important for FAGECOMB -> Not having a father is predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def process_dataset(df, stats, scaler=None, is_train=False):\n",
    "   df = df.copy()\n",
    "\n",
    "   # A. Drop useless columns\n",
    "   df = df.drop([c for c in drop_cols if c in df.columns], axis=1)\n",
    "\n",
    "   # A.2 Feature Engineering\n",
    "   # 1. Peso total de la madre (Física)\n",
    "   if 'PWgt_R' in df.columns and 'WTGAIN' in df.columns:\n",
    "        df['Mother_Total_Weight'] = df['PWgt_R'] + df['WTGAIN']\n",
    "\n",
    "    # 2. Intensidad de tabaquismo (Interacción)\n",
    "   if 'CIG_0' in df.columns and 'MAGER' in df.columns:\n",
    "        df['Smoking_Intensity'] = df['CIG_0'] * df['MAGER']\n",
    "\n",
    "   # B. Missing indicators\n",
    "   cols_with_heavy_nan = ['FAGECOMB']\n",
    "   for col in cols_with_heavy_nan:\n",
    "      if col in df.columns:\n",
    "         # Creates a binary column: 1 if missing, 0 if not\n",
    "         df[f'{col}_is_missing'] = df[col].isna().astype(int)\n",
    "\n",
    "   # C. Imputation (fill in the blanks)\n",
    "   for col, value in stats.items():\n",
    "      if col in df.columns:\n",
    "         df[col] = df[col].fillna(value)\n",
    "\n",
    "   # Security check: if any NaN remains\n",
    "   df = df.fillna(0)\n",
    "\n",
    "   # D. One-hot encoding for categorical variables to dummy\n",
    "   # Change to string first to secure numerical codes are treated as categories\n",
    "   current_cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "   for col in current_cat_cols:\n",
    "      df[col] = df[col].astype(str)\n",
    "\n",
    "   df = pd.get_dummies(df, columns=current_cat_cols, drop_first=True) # drop_first avoids colineality\n",
    "\n",
    "   # E. Scaling (StandardScaler)\n",
    "   current_num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "   if is_train:\n",
    "      scaler = StandardScaler()\n",
    "      df[current_num_cols] = scaler.fit_transform(df[current_num_cols])\n",
    "      return df, scaler\n",
    "   else:\n",
    "      # if Val/Test we use the scaler already trained (NOT RE-FIT)\n",
    "      if scaler is not None:\n",
    "         df[current_num_cols] = scaler.transform(df[current_num_cols])\n",
    "      return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline execution\n",
    "1. Process TRAIN & obtain adjusted scaler\n",
    "2. Process VALIDATION using stats & Train scaler\n",
    "3. Process TEST\n",
    "4. PROCESS TEST for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "X_train_ready, scaler_fitted = process_dataset(X_train, imputation_stats, is_train=True)\n",
    "# 2\n",
    "X_val_ready = process_dataset(X_val, imputation_stats, scaler=scaler_fitted, is_train=False)\n",
    "# 3\n",
    "X_test_ready = process_dataset(X_test, imputation_stats, scaler=scaler_fitted, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding in the **D** step can generate distinct cols if in test missis some weird category. I force everyone to have EXACTLY the cols of X_train_ready\n",
    "expected_cols = X_train_ready.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_cols = X_train_ready.columns\n",
    "\n",
    "def align_columns(df, target_cols):\n",
    "    # 1. Add missing cols (rellenas con 0)\n",
    "    missing_cols = set(target_cols) - set(df.columns)\n",
    "    for c in missing_cols:\n",
    "        df[c] = 0\n",
    "\n",
    "    # 2. Remove extra columns (that weren't in train)\n",
    "    # 3. Reordenar para que coincidan índice a índice\n",
    "    return df[target_cols]\n",
    "\n",
    "X_val_ready = align_columns(X_val_ready, expected_cols)\n",
    "X_test_ready = align_columns(X_test_ready, expected_cols)\n",
    "\n",
    "print(f\"Train final shape: {X_train_ready.shape}\")\n",
    "print(f\"Val final shape:   {X_val_ready.shape}\")\n",
    "print(f\"Test final shape:  {X_test_ready.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Entrenar\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_ready, y_train)\n",
    "\n",
    "# Predecir y evaluar\n",
    "preds = model.predict(X_val_ready)\n",
    "rmse = root_mean_squared_error(y_val, preds)\n",
    "\n",
    "print(f\"Linear Regression RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# 1. Definir el espacio de búsqueda (Hyperparameter Space)\n",
    "# Estos son los \"knobs to twiddle\" mencionados en el Cap. 10\n",
    "param_dist = {\n",
    "    'n_estimators': randint(500, 3000),      # Cuántos árboles (Sección 10.4)\n",
    "    'learning_rate': uniform(0.01, 0.2),     # Velocidad de aprendizaje (Sección 10.5)\n",
    "    'max_depth': randint(3, 10),             # Profundidad de los árboles (Sección 10.3)\n",
    "    'subsample': uniform(0.6, 0.4),          # Evitar overfitting muestreando filas\n",
    "    'colsample_bytree': uniform(0.6, 0.4),   # Evitar overfitting muestreando columnas\n",
    "    'reg_alpha': uniform(0, 10),             # Regularización L1 (Sección 7.13.1)\n",
    "    'reg_lambda': uniform(0, 10)             # Regularización L2\n",
    "}\n",
    "\n",
    "# 2. Inicializar el regresor base\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# 3. Configurar la Búsqueda Aleatoria\n",
    "# n_iter=50 significa que probará 50 combinaciones distintas\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error', # Kaggle usa RMSE\n",
    "    cv=3,                                  # Validación cruzada de 3 pliegues (Sección 5.2)\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 4. Entrenar (¡Esto buscará el mejor modelo posible!)\n",
    "print(\"Buscando los mejores hiperparámetros (paciencia)...\")\n",
    "random_search.fit(X_train_ready, y_train)\n",
    "\n",
    "# 5. Resultados\n",
    "best_model = random_search.best_estimator_\n",
    "print(f\"\\nMejores parámetros encontrados: {random_search.best_params_}\")\n",
    "print(f\"Mejor Score (RMSE CV negativo): {-random_search.best_score_:.2f}\")\n",
    "\n",
    "# 6. Validación final con tu Hold-out Set\n",
    "preds_val = best_model.predict(X_val_ready)\n",
    "rmse_val = root_mean_squared_error(y_val, preds_val)\n",
    "print(f\"RMSE Final en Validación: {rmse_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the true kaggle test\n",
    "test_kaggle_ready = process_dataset(test_cleaned, imputation_stats, scaler=scaler_fitted, is_train=False)\n",
    "test_kaggle_ready = align_columns(test_kaggle_ready, expected_cols)\n",
    "\n",
    "print(f\"Test Kaggle shape: {test_kaggle_ready.shape}\") # Debería ser (2000, 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NUEVA CELDA PARA ENSEMBLING Y SUBMISSION - VERSIÓN XGBOOST MODERNO]\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- 1. ENTRENANDO XGBOOST ---\")\n",
    "\n",
    "# CORRECCIÓN: 'early_stopping_rounds' se define AHORA, al crear el modelo\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror',\n",
    "    early_stopping_rounds=50  # <--- AQUI ES DONDE DEBE IR EN VERSIONES NUEVAS\n",
    ")\n",
    "\n",
    "# En el .fit() ya NO ponemos early_stopping_rounds, pero SÍ mantenemos el eval_set\n",
    "# para que el modelo tenga datos con los que medir cuándo parar.\n",
    "model_xgb.fit(\n",
    "    X_train_ready, y_train,\n",
    "    eval_set=[(X_val_ready, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"XGBoost entrenado. Mejor iteración: {model_xgb.best_iteration}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 2. ENTRENANDO LIGHTGBM ---\")\n",
    "model_lgb = lgb.LGBMRegressor(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=31,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_fraction=0.7,\n",
    "    bagging_freq=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# LightGBM usa callbacks (esto sigue igual, es correcto)\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "\n",
    "model_lgb.fit(\n",
    "    X_train_ready, y_train,\n",
    "    eval_set=[(X_val_ready, y_val)],\n",
    "    eval_metric='rmse',\n",
    "    callbacks=callbacks\n",
    ")\n",
    "print(f\"LightGBM entrenado. Mejor iteración: {model_lgb.best_iteration_}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. EVALUACIÓN INTERNA (BLENDING) ---\")\n",
    "# Predecimos (automáticamente usan la mejor iteración gracias al early stopping)\n",
    "val_preds_xgb = model_xgb.predict(X_val_ready)\n",
    "val_preds_lgb = model_lgb.predict(X_val_ready)\n",
    "\n",
    "# PROMEDIO SIMPLE\n",
    "val_preds_ensemble = (val_preds_xgb * 0.5) + (val_preds_lgb * 0.5)\n",
    "\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_val, val_preds_xgb))\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_val, val_preds_lgb))\n",
    "rmse_ens = np.sqrt(mean_squared_error(y_val, val_preds_ensemble))\n",
    "\n",
    "print(f\"RMSE XGBoost:  {rmse_xgb:.2f}\")\n",
    "print(f\"RMSE LightGBM: {rmse_lgb:.2f}\")\n",
    "print(f\"RMSE ENSAMBLE: {rmse_ens:.2f} (¡Tu referencia real!)\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 4. GENERANDO SUBMISSION PARA KAGGLE ---\")\n",
    "# Predicciones finales sobre el test de competición\n",
    "kaggle_preds_xgb = model_xgb.predict(test_kaggle_ready)\n",
    "kaggle_preds_lgb = model_lgb.predict(test_kaggle_ready)\n",
    "\n",
    "# Blending final\n",
    "final_predictions = (kaggle_preds_xgb * 0.5) + (kaggle_preds_lgb * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- CONFIGURANDO EL APILAMIENTO (STACKING) ---\")\n",
    "\n",
    "# 1. Definir los modelos base (Base Learners)\n",
    "# Usamos configuraciones robustas para cada uno\n",
    "estimators = [\n",
    "    ('xgb', xgb.XGBRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,  # Más lento = más precisión\n",
    "        max_depth=6,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        objective='reg:squarederror',\n",
    "        # Ojo: StackingRegressor maneja el fit internamente, no usamos early_stopping aquí\n",
    "        # para simplificar la compatibilidad, pero compensamos con learning_rate bajo.\n",
    "    )),\n",
    "    ('lgb', lgb.LGBMRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=31,\n",
    "        feature_fraction=0.7,\n",
    "        bagging_fraction=0.7,\n",
    "        bagging_freq=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )),\n",
    "    ('cat', CatBoostRegressor(\n",
    "        iterations=2000,\n",
    "        learning_rate=0.01,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,       # Regularización específica de CatBoost\n",
    "        verbose=0,           # Silencioso\n",
    "        random_state=42\n",
    "    ))\n",
    "]\n",
    "\n",
    "# 2. Definir el Meta-Modelo (Final Estimator)\n",
    "# RidgeCV es una regresión lineal con regularización L2 integrada (Sección 7.13.1)\n",
    "# Es ideal para combinar predicciones porque maneja la colinealidad.\n",
    "meta_model = RidgeCV()\n",
    "\n",
    "# 3. Construir el Stacking Regressor\n",
    "# cv=5 asegura que las predicciones intermedias se generen con validación cruzada (Sección 9.8)\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False # False = El meta-modelo solo ve las predicciones, no los datos originales\n",
    ")\n",
    "\n",
    "print(\"Entrenando Stacking Regressor (esto tardará un poco)...\")\n",
    "# Entrenamos en TODO el conjunto de entrenamiento disponible\n",
    "stacking_model.fit(X_train_ready, y_train)\n",
    "\n",
    "print(\"¡Entrenamiento completado!\")\n",
    "\n",
    "# --- EVALUACIÓN ---\n",
    "print(\"\\n--- EVALUACIÓN EN VALIDACIÓN ---\")\n",
    "val_preds = stacking_model.predict(X_val_ready)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "print(f\"RMSE Stacking (Train/Val split): {rmse_val:.2f}\")\n",
    "\n",
    "# --- GENERACIÓN DE SUBMISSION ---\n",
    "print(\"\\n--- GENERANDO SUBMISSION FINAL ---\")\n",
    "kaggle_preds = stacking_model.predict(test_kaggle_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions = model.predict(test_kaggle_ready)\n",
    "kaggle_predictions_xgb = best_model.predict(test_kaggle_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your predictions in a `submission.csv` file for scoring on the [leaderboard](https://www.kaggle.com/competitions/u-tad-birth-weight-point-prediction-2025/leaderboard)\n",
    "To submit your notebook click on **Submit to competition** and then **Submit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T16:14:19.922232Z",
     "iopub.status.busy": "2025-10-06T16:14:19.921888Z",
     "iopub.status.idle": "2025-10-06T16:14:19.936268Z",
     "shell.execute_reply": "2025-10-06T16:14:19.935662Z",
     "shell.execute_reply.started": "2025-10-06T16:14:19.922198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# do not modify this code\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission[\"DBWT\"] = final_predictions\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13892590,
     "sourceId": 116374,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter notebook\n",
    "Copy (fork) and edit as many copies of this notebook as you require "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-06T15:56:36.745832Z",
     "iopub.status.busy": "2025-10-06T15:56:36.745119Z",
     "iopub.status.idle": "2025-10-06T15:56:38.744392Z",
     "shell.execute_reply": "2025-10-06T15:56:38.743657Z",
     "shell.execute_reply.started": "2025-10-06T15:56:36.745764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 72)\n",
    "\n",
    "seed = 42\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 6.8)\n",
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1)\n",
    "#sns.set_style(\"whitegrid\")\n",
    "\n",
    "import plotly.io as pio\n",
    "# for use in JupyterLab 4\n",
    "pio.renderers.default = 'iframe'\n",
    "# for use in Google Colab\n",
    "#pio.renderers.default = 'colab'\n",
    "import plotly as py\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:56:38.746407Z",
     "iopub.status.busy": "2025-10-06T15:56:38.745959Z",
     "iopub.status.idle": "2025-10-06T15:56:38.841396Z",
     "shell.execute_reply": "2025-10-06T15:56:38.840545Z",
     "shell.execute_reply.started": "2025-10-06T15:56:38.746378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ATTEND</th>\n",
       "      <th>BFACIL</th>\n",
       "      <th>BMI</th>\n",
       "      <th>CIG_0</th>\n",
       "      <th>DLMP_MM</th>\n",
       "      <th>DMAR</th>\n",
       "      <th>DOB_MM</th>\n",
       "      <th>DOB_TT</th>\n",
       "      <th>DOB_WK</th>\n",
       "      <th>FAGECOMB</th>\n",
       "      <th>FEDUC</th>\n",
       "      <th>ILLB_R</th>\n",
       "      <th>ILOP_R</th>\n",
       "      <th>ILP_R</th>\n",
       "      <th>LD_INDL</th>\n",
       "      <th>MAGER</th>\n",
       "      <th>MBSTATE_REC</th>\n",
       "      <th>MEDUC</th>\n",
       "      <th>M_Ht_In</th>\n",
       "      <th>NO_INFEC</th>\n",
       "      <th>NO_MMORB</th>\n",
       "      <th>NO_RISKS</th>\n",
       "      <th>PAY</th>\n",
       "      <th>PAY_REC</th>\n",
       "      <th>PRECARE</th>\n",
       "      <th>PREVIS</th>\n",
       "      <th>PRIORDEAD</th>\n",
       "      <th>PRIORLIVE</th>\n",
       "      <th>PRIORTERM</th>\n",
       "      <th>PWgt_R</th>\n",
       "      <th>RDMETH_REC</th>\n",
       "      <th>RESTATUS</th>\n",
       "      <th>RF_CESAR</th>\n",
       "      <th>RF_CESARN</th>\n",
       "      <th>SEX</th>\n",
       "      <th>WTGAIN</th>\n",
       "      <th>DBWT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>N</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>3572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>712.0</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>Y</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>173.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>35</td>\n",
       "      <td>3315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>853.0</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>888</td>\n",
       "      <td>29</td>\n",
       "      <td>N</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2054.0</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>888</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>N</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>31</td>\n",
       "      <td>3005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>944.0</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>N</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>3714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>6995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>20</td>\n",
       "      <td>3374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>6996</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1757.0</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>888</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>N</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>215.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>29</td>\n",
       "      <td>4195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>6997</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>44.6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>N</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>6998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>888</td>\n",
       "      <td>15</td>\n",
       "      <td>N</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>3810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>6999</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>250.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>888</td>\n",
       "      <td>39</td>\n",
       "      <td>N</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>28</td>\n",
       "      <td>3720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  ATTEND  BFACIL   BMI  CIG_0  DLMP_MM DMAR  DOB_MM  DOB_TT  DOB_WK  \\\n",
       "0        0       1       1  21.3      0        7    1       5  1319.0       3   \n",
       "1        1       1       1  27.1      0       12    2       9   712.0       3   \n",
       "2        2       1       1  27.8      0        4    1      12   853.0       5   \n",
       "3        3       1       1  22.3      0       12    1       9  2054.0       2   \n",
       "4        4       1       1  22.7      0        2    1      11   944.0       3   \n",
       "...    ...     ...     ...   ...    ...      ...  ...     ...     ...     ...   \n",
       "6995  6995       1       1  20.2      0       11    1       8  1251.0       5   \n",
       "6996  6996       1       1  36.9      0        8    1       5  1757.0       4   \n",
       "6997  6997       3       1  44.6      0        9            6  1746.0       7   \n",
       "6998  6998       1       1  24.4      0        1    1      10     NaN       3   \n",
       "6999  6999       1       1  20.1      0        9    1       7   250.0       2   \n",
       "\n",
       "      FAGECOMB  FEDUC  ILLB_R  ILOP_R  ILP_R LD_INDL  MAGER  MBSTATE_REC  \\\n",
       "0           28      6     888     888    888       N     25            1   \n",
       "1           99      9      40     999    999       Y     26            1   \n",
       "2           25      4      29     888     29       N     25            1   \n",
       "3           38      6     888      22     22       N     38            1   \n",
       "4           29      4     999     999    999       N     30            1   \n",
       "...        ...    ...     ...     ...    ...     ...    ...          ...   \n",
       "6995        30      6      33      44     33       N     28            1   \n",
       "6996        37      5     888      36     36       N     31            1   \n",
       "6997        20      4     888     888    888       N     18            1   \n",
       "6998        33      6      15     888     15       N     32            1   \n",
       "6999        40      6      39     888     39       N     34            1   \n",
       "\n",
       "      MEDUC  M_Ht_In  NO_INFEC  NO_MMORB  NO_RISKS  PAY  PAY_REC  PRECARE  \\\n",
       "0         6       68         1         1         1    2        2        2   \n",
       "1         3       67         1         1         0    1        1        2   \n",
       "2         4       63         1         1         0    2        2        4   \n",
       "3         6       63         1         1         1    2        2        2   \n",
       "4         4       64         1         1         1    2        2        2   \n",
       "...     ...      ...       ...       ...       ...  ...      ...      ...   \n",
       "6995      6       66         1         1         1    2        2        2   \n",
       "6996      6       64         1         1         1    2        2        2   \n",
       "6997      3       64         1         1         1    2        2        2   \n",
       "6998      6       70         1         1         0    2        2        2   \n",
       "6999      8       68         1         1         1    2        2        3   \n",
       "\n",
       "      PREVIS  PRIORDEAD  PRIORLIVE  PRIORTERM  PWgt_R  RDMETH_REC  RESTATUS  \\\n",
       "0         13          0          0          0   140.0           1         2   \n",
       "1         13          0          2          3   173.0           1         1   \n",
       "2          8          0          2          0   157.0           3         1   \n",
       "3         13          0          0          1   126.0           1         1   \n",
       "4         12          0          2          1     NaN           1         2   \n",
       "...      ...        ...        ...        ...     ...         ...       ...   \n",
       "6995       9          0          1          1   125.0           1         2   \n",
       "6996      15          0          0          1   215.0           3         2   \n",
       "6997       9          0          0          0   260.0           1         1   \n",
       "6998      17          0          1          0   170.0           4         3   \n",
       "6999       9          0          1          0   132.0           1         1   \n",
       "\n",
       "     RF_CESAR  RF_CESARN SEX  WTGAIN  DBWT  \n",
       "0           N          0   F      37  3572  \n",
       "1           N          0   F      35  3315  \n",
       "2           N          0   M      41  1910  \n",
       "3           N          0   F      31  3005  \n",
       "4           N          0   F      18  3714  \n",
       "...       ...        ...  ..     ...   ...  \n",
       "6995        N          0   M      20  3374  \n",
       "6996        N          0   F      29  4195  \n",
       "6997        N          0   F       0  2900  \n",
       "6998        Y          1   F      45  3810  \n",
       "6999        N          0   F      28  3720  \n",
       "\n",
       "[7000 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the test data that you are asked to make predictions for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T15:57:05.885591Z",
     "iopub.status.busy": "2025-10-06T15:57:05.884831Z",
     "iopub.status.idle": "2025-10-06T15:57:05.924461Z",
     "shell.execute_reply": "2025-10-06T15:57:05.923725Z",
     "shell.execute_reply.started": "2025-10-06T15:57:05.88556Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ATTEND</th>\n",
       "      <th>BFACIL</th>\n",
       "      <th>BMI</th>\n",
       "      <th>CIG_0</th>\n",
       "      <th>DLMP_MM</th>\n",
       "      <th>DMAR</th>\n",
       "      <th>DOB_MM</th>\n",
       "      <th>DOB_TT</th>\n",
       "      <th>DOB_WK</th>\n",
       "      <th>FAGECOMB</th>\n",
       "      <th>FEDUC</th>\n",
       "      <th>ILLB_R</th>\n",
       "      <th>ILOP_R</th>\n",
       "      <th>ILP_R</th>\n",
       "      <th>LD_INDL</th>\n",
       "      <th>MAGER</th>\n",
       "      <th>MBSTATE_REC</th>\n",
       "      <th>MEDUC</th>\n",
       "      <th>M_Ht_In</th>\n",
       "      <th>NO_INFEC</th>\n",
       "      <th>NO_MMORB</th>\n",
       "      <th>NO_RISKS</th>\n",
       "      <th>PAY</th>\n",
       "      <th>PAY_REC</th>\n",
       "      <th>PRECARE</th>\n",
       "      <th>PREVIS</th>\n",
       "      <th>PRIORDEAD</th>\n",
       "      <th>PRIORLIVE</th>\n",
       "      <th>PRIORTERM</th>\n",
       "      <th>PWgt_R</th>\n",
       "      <th>RDMETH_REC</th>\n",
       "      <th>RESTATUS</th>\n",
       "      <th>RF_CESAR</th>\n",
       "      <th>RF_CESARN</th>\n",
       "      <th>SEX</th>\n",
       "      <th>WTGAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>412.0</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>888</td>\n",
       "      <td>29</td>\n",
       "      <td>Y</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>N</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>N</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>N</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37.1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>888</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>N</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>237.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>8995</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>888</td>\n",
       "      <td>16</td>\n",
       "      <td>N</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>8996</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>888</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>Y</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>8997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>888</td>\n",
       "      <td>54</td>\n",
       "      <td>N</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>8998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>636.0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>888</td>\n",
       "      <td>74</td>\n",
       "      <td>N</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>8999</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37.9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>888</td>\n",
       "      <td>N</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  ATTEND  BFACIL   BMI  CIG_0  DLMP_MM DMAR  DOB_MM  DOB_TT  DOB_WK  \\\n",
       "0     7000       1       1  35.7      0       12    2       9   412.0       3   \n",
       "1     7001       1       1  24.2      0       11            8  1007.0       7   \n",
       "2     7002       1       1  20.2      0        5    2       2  1328.0       5   \n",
       "3     7003       1       1  23.0      0        8            5  2012.0       2   \n",
       "4     7004       1       1  37.1      0        3    2      12     NaN       2   \n",
       "...    ...     ...     ...   ...    ...      ...  ...     ...     ...     ...   \n",
       "1995  8995       2       1  20.4      0       11    1       8  1653.0       3   \n",
       "1996  8996       1       1  26.2      0        7    1       4  1021.0       6   \n",
       "1997  8997       1       1  35.9      0        9    1       6  1014.0       1   \n",
       "1998  8998       1       1  19.0      0       12    2       9   636.0       2   \n",
       "1999  8999       1       1  37.9      0        3    1      12  1125.0       6   \n",
       "\n",
       "      FAGECOMB  FEDUC  ILLB_R  ILOP_R  ILP_R LD_INDL  MAGER  MBSTATE_REC  \\\n",
       "0           38      4      29     888     29       Y     34            1   \n",
       "1           24      3     888     888    888       N     22            1   \n",
       "2           31      5     888     888    888       N     26            2   \n",
       "3           22      4     888     888    888       N     19            1   \n",
       "4           33      4     888     999    999       N     33            1   \n",
       "...        ...    ...     ...     ...    ...     ...    ...          ...   \n",
       "1995        29      7      16     888     16       N     32            1   \n",
       "1996        29      4     888     999    999       Y     27            1   \n",
       "1997        56      6      54     888     54       N     44            2   \n",
       "1998        29      6      74     888     74       N     28            1   \n",
       "1999        39      5     888     888    888       N     38            1   \n",
       "\n",
       "      MEDUC  M_Ht_In  NO_INFEC  NO_MMORB  NO_RISKS  PAY  PAY_REC  PRECARE  \\\n",
       "0         6       71         1         1         1    2        2        3   \n",
       "1         3       64         1         1         1    2        2        2   \n",
       "2         7       69         1         1         1    1        1        2   \n",
       "3         5       63         1         1         1    1        1        2   \n",
       "4         6       67         1         1         0    1        1        3   \n",
       "...     ...      ...       ...       ...       ...  ...      ...      ...   \n",
       "1995      4       67         1         1         1    2        2        3   \n",
       "1996      6       67         1         1         1    1        1        2   \n",
       "1997      6       68         1         1         0    2        2        2   \n",
       "1998      4       66         1         1         1    1        1        2   \n",
       "1999      5       69         1         1         1    2        2        2   \n",
       "\n",
       "      PREVIS  PRIORDEAD  PRIORLIVE  PRIORTERM  PWgt_R  RDMETH_REC  RESTATUS  \\\n",
       "0         11          0          1          0   256.0           1         2   \n",
       "1         11          0          0          0   141.0           1         1   \n",
       "2         16          0          0          0   137.0           3         2   \n",
       "3         10          0          0          0   130.0           1         1   \n",
       "4         13          0          0          2   237.0           3         1   \n",
       "...      ...        ...        ...        ...     ...         ...       ...   \n",
       "1995       7          0          4          0   130.0           1         3   \n",
       "1996      16          0          0          1   167.0           1         1   \n",
       "1997      11          0          1          0   236.0           4         1   \n",
       "1998      12          0          1          0   118.0           1         1   \n",
       "1999      16          0          0          0   257.0           3         2   \n",
       "\n",
       "     RF_CESAR  RF_CESARN SEX  WTGAIN  \n",
       "0           N          0   M      24  \n",
       "1           N          0   F      11  \n",
       "2           N          0   M      39  \n",
       "3           N          0   M      43  \n",
       "4           N          0   F      30  \n",
       "...       ...        ...  ..     ...  \n",
       "1995        N          0   F      18  \n",
       "1996        N          0   M      54  \n",
       "1997        Y          1   F      36  \n",
       "1998        N          0   F      41  \n",
       "1999        N          0   M      40  \n",
       "\n",
       "[2000 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural cleaning before splitting\n",
    "def structural_cleaning(df, is_train=True):\n",
    "   df = df.copy()\n",
    "\n",
    "   # A. Filter target (only train)\n",
    "   if is_train and 'DBWT' in df.columns:\n",
    "       df = df[df['DBWT'] != 9999]\n",
    "\n",
    "   # B. Null codes\n",
    "   # Map 99, 999 ...\n",
    "   cols_99 = ['CIG_0', 'FAGECOMB', 'M_Ht_In', 'PREVIS']\n",
    "   for col in cols_99:\n",
    "       if col in df.columns:\n",
    "           df[col] = df[col].replace(99, np.nan)\n",
    "\n",
    "   cols_9 = ['FEDUC', 'MEDUC', 'BFACIL', 'ATTEND']\n",
    "   for col in cols_9:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(9, np.nan)\n",
    "\n",
    "   if 'BMI' in df.columns:\n",
    "        df['BMI'] = df['BMI'].replace(99.9, np.nan)\n",
    "\n",
    "   # C. Structural nullity (888 -> 0)\n",
    "   cols_888 = ['ILLB_R', 'ILOP_R', 'ILP_R']\n",
    "   for col in cols_888:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(888, 0)\n",
    "            df[col] = df[col].replace(999, np.nan)\n",
    "\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply structural cleaning to every df\n",
    "train_cleaned = structural_cleaning(train, is_train=True)\n",
    "test_cleaned = structural_cleaning(test, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting is based into 3 conjuntos\n",
    "1. Conjunto train: Is used so the model can learn parameters\n",
    "2. Conjunto validation: To select the best model and set hyperparameters to estimate errors of generalization during development.\n",
    "3. Conjunto test: Untouchable until the end. Gives a non-biassed estimation about how the model works irl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (4200, 37) (aprox 60%)\n",
      "Val shape:   (1400, 37)   (aprox 20%)\n",
      "Test shape:  (1400, 37)  (aprox 20%)\n"
     ]
    }
   ],
   "source": [
    "# The target variable is 'Delivery Birth Weight' (DBWT)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate target variable to isolate the features (X)\n",
    "y = train_cleaned.pop('DBWT')\n",
    "X = train_cleaned\n",
    "\n",
    "# 80/20 ratio\n",
    "# Split the 20% of data for final testing\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# From the 80% training data, split again into training and validation sets (75/25 ratio)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape} (aprox 60%)\")\n",
    "print(f\"Val shape:   {X_val.shape}   (aprox 20%)\")\n",
    "print(f\"Test shape:  {X_test.shape}  (aprox 20%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a baseline signifies that every model turned in after this must best this result no matter what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline value: 3259.56\n",
      "Baseline RMSE on validation set: 581.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Define the prediction (mean of training target)\n",
    "baseline_pred = y_train.mean()\n",
    "print(f\"Baseline value: {baseline_pred:.2f}\")\n",
    "\n",
    "# Create a dummy prediction vector array for validation\n",
    "y_val_pred = np.full(len(y_val), baseline_pred)\n",
    "\n",
    "# Evaluate RMSE\n",
    "rmse = root_mean_squared_error(y_val, y_val_pred)\n",
    "print(f\"Baseline RMSE on validation set: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical imputation is the process of replacing missing data points in a dataset with plausible substituted values to maintaint data integrity. I did the splitting before filling in data to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting all the info from `UserGuide2018-508` shared in the Kaggle description I made a full data cleanup to try a standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that do not give predictive value\n",
    "drop_cols = ['id', 'DLMP_MM', 'DOB_TT'] # Exact dates that give noise\n",
    "\n",
    "# Numerical columns -> Candidates for median + scaling\n",
    "# Education has order: We try as a numerical\n",
    "num_cols = [\n",
    "    'BMI', 'CIG_0', 'FAGECOMB', 'ILLB_R', 'ILOP_R', 'ILP_R', 'MAGER',\n",
    "    'M_Ht_In', 'PRECARE', 'PREVIS', 'PRIORDEAD', 'PRIORLIVE', 'PRIORTERM',\n",
    "    'PWgt_R', 'RF_CESARN', 'WTGAIN', 'FEDUC', 'MEDUC'\n",
    "]\n",
    "\n",
    "# Categorical columns -> Candidates for mode + one-hot encoding\n",
    "# Text variables that do not have order\n",
    "cat_cols = [\n",
    "    'ATTEND', 'BFACIL', 'DMAR', 'DOB_MM', 'DOB_WK', 'LD_INDL',\n",
    "    'MBSTATE_REC', 'PAY', 'PAY_REC', 'RDMETH_REC', 'RESTATUS',\n",
    "    'RF_CESAR', 'SEX', 'NO_INFEC', 'NO_MMORB', 'NO_RISKS'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training stats learning\n",
    "def learn_imputation_stats(df):\n",
    "   stats = {}\n",
    "\n",
    "   # A. Numerical -> Median\n",
    "   for col in num_cols:\n",
    "      if col in df.columns:\n",
    "         stats[col] = df[col].median()\n",
    "\n",
    "   # B. Categorical -> Mode\n",
    "   for col in cat_cols:\n",
    "      if col in df.columns:\n",
    "         # dropna=True to avoid NaN in mode calculation\n",
    "         stats[col] = df[col].mode(dropna=True)[0]\n",
    "\n",
    "   return stats\n",
    "imputation_stats = learn_imputation_stats(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Really important for FAGECOMB -> Not having a father is predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def process_dataset(df, stats, scaler=None, is_train=False):\n",
    "   df = df.copy()\n",
    "\n",
    "   # A. Drop useless columns\n",
    "   df = df.drop([c for c in drop_cols if c in df.columns], axis=1)\n",
    "\n",
    "   # A.2 Feature Engineering\n",
    "   # 1. Peso total de la madre (Física)\n",
    "   if 'PWgt_R' in df.columns and 'WTGAIN' in df.columns:\n",
    "        df['Mother_Total_Weight'] = df['PWgt_R'] + df['WTGAIN']\n",
    "\n",
    "    # 2. Intensidad de tabaquismo (Interacción)\n",
    "   if 'CIG_0' in df.columns and 'MAGER' in df.columns:\n",
    "        df['Smoking_Intensity'] = df['CIG_0'] * df['MAGER']\n",
    "\n",
    "   # B. Missing indicators\n",
    "   cols_with_heavy_nan = ['FAGECOMB']\n",
    "   for col in cols_with_heavy_nan:\n",
    "      if col in df.columns:\n",
    "         # Creates a binary column: 1 if missing, 0 if not\n",
    "         df[f'{col}_is_missing'] = df[col].isna().astype(int)\n",
    "\n",
    "   # C. Imputation (fill in the blanks)\n",
    "   for col, value in stats.items():\n",
    "      if col in df.columns:\n",
    "         df[col] = df[col].fillna(value)\n",
    "\n",
    "   # Security check: if any NaN remains\n",
    "   df = df.fillna(0)\n",
    "\n",
    "   # D. One-hot encoding for categorical variables to dummy\n",
    "   # Change to string first to secure numerical codes are treated as categories\n",
    "   current_cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "   for col in current_cat_cols:\n",
    "      df[col] = df[col].astype(str)\n",
    "\n",
    "   df = pd.get_dummies(df, columns=current_cat_cols, drop_first=True) # drop_first avoids colineality\n",
    "\n",
    "   # E. Scaling (StandardScaler)\n",
    "   current_num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "   if is_train:\n",
    "      scaler = StandardScaler()\n",
    "      df[current_num_cols] = scaler.fit_transform(df[current_num_cols])\n",
    "      return df, scaler\n",
    "   else:\n",
    "      # if Val/Test we use the scaler already trained (NOT RE-FIT)\n",
    "      if scaler is not None:\n",
    "         df[current_num_cols] = scaler.transform(df[current_num_cols])\n",
    "      return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline execution\n",
    "1. Process TRAIN & obtain adjusted scaler\n",
    "2. Process VALIDATION using stats & Train scaler\n",
    "3. Process TEST\n",
    "4. PROCESS TEST for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "X_train_ready, scaler_fitted = process_dataset(X_train, imputation_stats, is_train=True)\n",
    "# 2\n",
    "X_val_ready = process_dataset(X_val, imputation_stats, scaler=scaler_fitted, is_train=False)\n",
    "# 3\n",
    "X_test_ready = process_dataset(X_test, imputation_stats, scaler=scaler_fitted, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding in the **D** step can generate distinct cols if in test missis some weird category. I force everyone to have EXACTLY the cols of X_train_ready\n",
    "expected_cols = X_train_ready.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train final shape: (4200, 76)\n",
      "Val final shape:   (1400, 76)\n",
      "Test final shape:  (1400, 76)\n"
     ]
    }
   ],
   "source": [
    "expected_cols = X_train_ready.columns\n",
    "\n",
    "def align_columns(df, target_cols):\n",
    "    # 1. Add missing cols (rellenas con 0)\n",
    "    missing_cols = set(target_cols) - set(df.columns)\n",
    "    for c in missing_cols:\n",
    "        df[c] = 0\n",
    "\n",
    "    # 2. Remove extra columns (that weren't in train)\n",
    "    # 3. Reordenar para que coincidan índice a índice\n",
    "    return df[target_cols]\n",
    "\n",
    "X_val_ready = align_columns(X_val_ready, expected_cols)\n",
    "X_test_ready = align_columns(X_test_ready, expected_cols)\n",
    "\n",
    "print(f\"Train final shape: {X_train_ready.shape}\")\n",
    "print(f\"Val final shape:   {X_val_ready.shape}\")\n",
    "print(f\"Test final shape:  {X_test_ready.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Entrenar\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_ready, y_train)\n",
    "\n",
    "# Predecir y evaluar\n",
    "preds = model.predict(X_val_ready)\n",
    "rmse = root_mean_squared_error(y_val, preds)\n",
    "\n",
    "print(f\"Linear Regression RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# 1. Definir el espacio de búsqueda (Hyperparameter Space)\n",
    "# Estos son los \"knobs to twiddle\" mencionados en el Cap. 10\n",
    "param_dist = {\n",
    "    'n_estimators': randint(500, 3000),      # Cuántos árboles (Sección 10.4)\n",
    "    'learning_rate': uniform(0.01, 0.2),     # Velocidad de aprendizaje (Sección 10.5)\n",
    "    'max_depth': randint(3, 10),             # Profundidad de los árboles (Sección 10.3)\n",
    "    'subsample': uniform(0.6, 0.4),          # Evitar overfitting muestreando filas\n",
    "    'colsample_bytree': uniform(0.6, 0.4),   # Evitar overfitting muestreando columnas\n",
    "    'reg_alpha': uniform(0, 10),             # Regularización L1 (Sección 7.13.1)\n",
    "    'reg_lambda': uniform(0, 10)             # Regularización L2\n",
    "}\n",
    "\n",
    "# 2. Inicializar el regresor base\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# 3. Configurar la Búsqueda Aleatoria\n",
    "# n_iter=50 significa que probará 50 combinaciones distintas\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error', # Kaggle usa RMSE\n",
    "    cv=3,                                  # Validación cruzada de 3 pliegues (Sección 5.2)\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 4. Entrenar (¡Esto buscará el mejor modelo posible!)\n",
    "print(\"Buscando los mejores hiperparámetros (paciencia)...\")\n",
    "random_search.fit(X_train_ready, y_train)\n",
    "\n",
    "# 5. Resultados\n",
    "best_model = random_search.best_estimator_\n",
    "print(f\"\\nMejores parámetros encontrados: {random_search.best_params_}\")\n",
    "print(f\"Mejor Score (RMSE CV negativo): {-random_search.best_score_:.2f}\")\n",
    "\n",
    "# 6. Validación final con tu Hold-out Set\n",
    "preds_val = best_model.predict(X_val_ready)\n",
    "rmse_val = root_mean_squared_error(y_val, preds_val)\n",
    "print(f\"RMSE Final en Validación: {rmse_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Kaggle shape: (2000, 76)\n"
     ]
    }
   ],
   "source": [
    "# Process the true kaggle test\n",
    "test_kaggle_ready = process_dataset(test_cleaned, imputation_stats, scaler=scaler_fitted, is_train=False)\n",
    "test_kaggle_ready = align_columns(test_kaggle_ready, expected_cols)\n",
    "\n",
    "print(f\"Test Kaggle shape: {test_kaggle_ready.shape}\") # Debería ser (2000, 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NUEVA CELDA PARA ENSEMBLING Y SUBMISSION - VERSIÓN XGBOOST MODERNO]\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- 1. ENTRENANDO XGBOOST ---\")\n",
    "\n",
    "# CORRECCIÓN: 'early_stopping_rounds' se define AHORA, al crear el modelo\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror',\n",
    "    early_stopping_rounds=50  # <--- AQUI ES DONDE DEBE IR EN VERSIONES NUEVAS\n",
    ")\n",
    "\n",
    "# En el .fit() ya NO ponemos early_stopping_rounds, pero SÍ mantenemos el eval_set\n",
    "# para que el modelo tenga datos con los que medir cuándo parar.\n",
    "model_xgb.fit(\n",
    "    X_train_ready, y_train,\n",
    "    eval_set=[(X_val_ready, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"XGBoost entrenado. Mejor iteración: {model_xgb.best_iteration}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 2. ENTRENANDO LIGHTGBM ---\")\n",
    "model_lgb = lgb.LGBMRegressor(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=31,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_fraction=0.7,\n",
    "    bagging_freq=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# LightGBM usa callbacks (esto sigue igual, es correcto)\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "\n",
    "model_lgb.fit(\n",
    "    X_train_ready, y_train,\n",
    "    eval_set=[(X_val_ready, y_val)],\n",
    "    eval_metric='rmse',\n",
    "    callbacks=callbacks\n",
    ")\n",
    "print(f\"LightGBM entrenado. Mejor iteración: {model_lgb.best_iteration_}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. EVALUACIÓN INTERNA (BLENDING) ---\")\n",
    "# Predecimos (automáticamente usan la mejor iteración gracias al early stopping)\n",
    "val_preds_xgb = model_xgb.predict(X_val_ready)\n",
    "val_preds_lgb = model_lgb.predict(X_val_ready)\n",
    "\n",
    "# PROMEDIO SIMPLE\n",
    "val_preds_ensemble = (val_preds_xgb * 0.5) + (val_preds_lgb * 0.5)\n",
    "\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_val, val_preds_xgb))\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_val, val_preds_lgb))\n",
    "rmse_ens = np.sqrt(mean_squared_error(y_val, val_preds_ensemble))\n",
    "\n",
    "print(f\"RMSE XGBoost:  {rmse_xgb:.2f}\")\n",
    "print(f\"RMSE LightGBM: {rmse_lgb:.2f}\")\n",
    "print(f\"RMSE ENSAMBLE: {rmse_ens:.2f} (¡Tu referencia real!)\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 4. GENERANDO SUBMISSION PARA KAGGLE ---\")\n",
    "# Predicciones finales sobre el test de competición\n",
    "kaggle_preds_xgb = model_xgb.predict(test_kaggle_ready)\n",
    "kaggle_preds_lgb = model_lgb.predict(test_kaggle_ready)\n",
    "\n",
    "# Blending final\n",
    "final_predictions = (kaggle_preds_xgb * 0.5) + (kaggle_preds_lgb * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:37:52,235] A new study created in memory with name: no-name-0c65c871-f0bc-4467-a2d0-6e6e955f2820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando optimización de XGBoost con Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 15:38:05,311] Trial 0 finished with value: 549.6170043945312 and parameters: {'n_estimators': 1467, 'learning_rate': 0.005588286029044617, 'max_depth': 10, 'subsample': 0.9524473956487312, 'colsample_bytree': 0.6411312458109759, 'reg_alpha': 2.2064196822955382e-05, 'reg_lambda': 1.992198562900269e-05}. Best is trial 0 with value: 549.6170043945312.\n",
      "[I 2025-12-03 15:38:10,850] Trial 1 finished with value: 544.0021769205729 and parameters: {'n_estimators': 1382, 'learning_rate': 0.006929374282017861, 'max_depth': 7, 'subsample': 0.9281414016670257, 'colsample_bytree': 0.5678982487568431, 'reg_alpha': 0.5769986938768169, 'reg_lambda': 0.5576458860404561}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:38:31,892] Trial 2 finished with value: 547.2861938476562 and parameters: {'n_estimators': 1907, 'learning_rate': 0.006469380523734172, 'max_depth': 8, 'subsample': 0.7526137417560274, 'colsample_bytree': 0.7414812877370376, 'reg_alpha': 0.07324681088952859, 'reg_lambda': 0.057352852253142045}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:39:00,253] Trial 3 finished with value: 551.3581949869791 and parameters: {'n_estimators': 1847, 'learning_rate': 0.015893729544337674, 'max_depth': 10, 'subsample': 0.7034831037458948, 'colsample_bytree': 0.8174450837211579, 'reg_alpha': 1.5138833175283075e-07, 'reg_lambda': 1.933925852643832e-05}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:39:12,618] Trial 4 finished with value: 561.5997924804688 and parameters: {'n_estimators': 3507, 'learning_rate': 0.021687581472319434, 'max_depth': 5, 'subsample': 0.9933876737860121, 'colsample_bytree': 0.5148528232995688, 'reg_alpha': 0.013184428833042352, 'reg_lambda': 6.406326400110922e-08}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:39:20,054] Trial 5 finished with value: 545.8423665364584 and parameters: {'n_estimators': 2016, 'learning_rate': 0.014101725611878856, 'max_depth': 3, 'subsample': 0.9826562433648265, 'colsample_bytree': 0.8838392774486037, 'reg_alpha': 5.533155139311914e-08, 'reg_lambda': 0.0008053234103135505}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:39:40,997] Trial 6 finished with value: 558.0863444010416 and parameters: {'n_estimators': 1939, 'learning_rate': 0.04525795273919845, 'max_depth': 8, 'subsample': 0.6766980786963264, 'colsample_bytree': 0.6931250979751165, 'reg_alpha': 4.19211814443466e-08, 'reg_lambda': 0.00016175614675973808}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:39:49,109] Trial 7 finished with value: 583.3975423177084 and parameters: {'n_estimators': 3982, 'learning_rate': 0.052643230279598434, 'max_depth': 3, 'subsample': 0.7965580491954498, 'colsample_bytree': 0.9780839166960379, 'reg_alpha': 9.00741176951763e-08, 'reg_lambda': 9.702661426976921e-08}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:39:55,456] Trial 8 finished with value: 551.248291015625 and parameters: {'n_estimators': 3699, 'learning_rate': 0.014215666787523486, 'max_depth': 3, 'subsample': 0.6281132399916901, 'colsample_bytree': 0.9339970072465915, 'reg_alpha': 2.4284900295381235, 'reg_lambda': 2.4061684088703212}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:40:00,451] Trial 9 finished with value: 550.3252970377604 and parameters: {'n_estimators': 1767, 'learning_rate': 0.013202759135667772, 'max_depth': 5, 'subsample': 0.6476307660540976, 'colsample_bytree': 0.8949049698437734, 'reg_alpha': 3.0104779944370175e-05, 'reg_lambda': 0.013173717949084061}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:40:04,852] Trial 10 finished with value: 569.1884358723959 and parameters: {'n_estimators': 1028, 'learning_rate': 0.09220041970422647, 'max_depth': 7, 'subsample': 0.5318983788847462, 'colsample_bytree': 0.522715398291625, 'reg_alpha': 8.116204457611982, 'reg_lambda': 8.805429358964181}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:40:11,764] Trial 11 finished with value: 550.5146280924479 and parameters: {'n_estimators': 2752, 'learning_rate': 0.009047497881979035, 'max_depth': 5, 'subsample': 0.8845111726074855, 'colsample_bytree': 0.8279472652246439, 'reg_alpha': 0.0016813208464177627, 'reg_lambda': 0.00993533565845926}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:40:20,329] Trial 12 finished with value: 549.0158081054688 and parameters: {'n_estimators': 2625, 'learning_rate': 0.008887106508551137, 'max_depth': 6, 'subsample': 0.873745416782132, 'colsample_bytree': 0.6108166926146985, 'reg_alpha': 4.48997958533921e-06, 'reg_lambda': 0.20865938331160283}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:40:31,087] Trial 13 finished with value: 553.1654459635416 and parameters: {'n_estimators': 1054, 'learning_rate': 0.027638483757053075, 'max_depth': 8, 'subsample': 0.8945886081118768, 'colsample_bytree': 0.8353894221434756, 'reg_alpha': 0.27590361429524785, 'reg_lambda': 0.001060199811687179}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:40:48,403] Trial 14 finished with value: 546.4457804361979 and parameters: {'n_estimators': 3047, 'learning_rate': 0.009008606533478186, 'max_depth': 4, 'subsample': 0.9881930626539597, 'colsample_bytree': 0.591592242127299, 'reg_alpha': 0.0013881659647592627, 'reg_lambda': 3.865947506027494e-06}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:41:04,210] Trial 15 finished with value: 545.3458862304688 and parameters: {'n_estimators': 2285, 'learning_rate': 0.005047933598387417, 'max_depth': 7, 'subsample': 0.8180081073089309, 'colsample_bytree': 0.7391319810887668, 'reg_alpha': 1.374841187341562e-06, 'reg_lambda': 0.6708753703444323}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:41:15,913] Trial 16 finished with value: 546.7100423177084 and parameters: {'n_estimators': 2452, 'learning_rate': 0.005291161014842004, 'max_depth': 7, 'subsample': 0.8190534756318147, 'colsample_bytree': 0.7615553579373697, 'reg_alpha': 2.4648156844838356e-06, 'reg_lambda': 0.4456916110595171}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:41:28,993] Trial 17 finished with value: 548.2988077799479 and parameters: {'n_estimators': 1429, 'learning_rate': 0.008551732481580412, 'max_depth': 9, 'subsample': 0.8275460242411531, 'colsample_bytree': 0.6858436965259217, 'reg_alpha': 7.138220576265792e-05, 'reg_lambda': 1.1859592345681689}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:41:39,110] Trial 18 finished with value: 546.4937133789062 and parameters: {'n_estimators': 2349, 'learning_rate': 0.006929899337107641, 'max_depth': 6, 'subsample': 0.913757367160573, 'colsample_bytree': 0.5771708385456631, 'reg_alpha': 1.018175617499003e-06, 'reg_lambda': 0.045359884599030696}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:42:10,938] Trial 19 finished with value: 559.4875895182291 and parameters: {'n_estimators': 3057, 'learning_rate': 0.024232612382241007, 'max_depth': 7, 'subsample': 0.7551167920989019, 'colsample_bytree': 0.7368447588383161, 'reg_alpha': 0.00039534304203733595, 'reg_lambda': 8.174573990942147}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:42:32,085] Trial 20 finished with value: 550.1612141927084 and parameters: {'n_estimators': 1374, 'learning_rate': 0.011631731860178069, 'max_depth': 9, 'subsample': 0.8454487919303888, 'colsample_bytree': 0.6597323137642259, 'reg_alpha': 0.02659442480013634, 'reg_lambda': 0.005761188648260727}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:42:41,308] Trial 21 finished with value: 559.5990804036459 and parameters: {'n_estimators': 2221, 'learning_rate': 0.01899931227620351, 'max_depth': 6, 'subsample': 0.9438773270477075, 'colsample_bytree': 0.9003022479992835, 'reg_alpha': 1.0323752631008881e-08, 'reg_lambda': 0.0004381747258604228}. Best is trial 1 with value: 544.0021769205729.\n",
      "[I 2025-12-03 15:42:45,828] Trial 22 finished with value: 543.0484619140625 and parameters: {'n_estimators': 2184, 'learning_rate': 0.007162775303461433, 'max_depth': 4, 'subsample': 0.9189594070329636, 'colsample_bytree': 0.7970678323472495, 'reg_alpha': 6.361476083792872e-07, 'reg_lambda': 0.27000192657777067}. Best is trial 22 with value: 543.0484619140625.\n",
      "[I 2025-12-03 15:42:51,721] Trial 23 finished with value: 545.0475667317709 and parameters: {'n_estimators': 2888, 'learning_rate': 0.0068102595613563814, 'max_depth': 4, 'subsample': 0.9387357692241354, 'colsample_bytree': 0.7787098887785485, 'reg_alpha': 5.36817755246826e-07, 'reg_lambda': 0.15915476694411682}. Best is trial 22 with value: 543.0484619140625.\n",
      "[I 2025-12-03 15:42:57,986] Trial 24 finished with value: 545.7020670572916 and parameters: {'n_estimators': 2931, 'learning_rate': 0.007222968516746161, 'max_depth': 4, 'subsample': 0.9450544686063076, 'colsample_bytree': 0.7848400791442256, 'reg_alpha': 4.0253658800080845e-07, 'reg_lambda': 0.10095845552617844}. Best is trial 22 with value: 543.0484619140625.\n",
      "[I 2025-12-03 15:43:05,825] Trial 25 finished with value: 551.583251953125 and parameters: {'n_estimators': 3386, 'learning_rate': 0.011058930193762948, 'max_depth': 4, 'subsample': 0.9164433214750618, 'colsample_bytree': 0.7944122513597318, 'reg_alpha': 0.4148081928719525, 'reg_lambda': 1.5834061843818865}. Best is trial 22 with value: 543.0484619140625.\n",
      "[I 2025-12-03 15:43:09,792] Trial 26 finished with value: 557.0454711914062 and parameters: {'n_estimators': 1634, 'learning_rate': 0.0324945843357146, 'max_depth': 4, 'subsample': 0.8689023206607839, 'colsample_bytree': 0.7038255358541821, 'reg_alpha': 6.202805000629396e-06, 'reg_lambda': 0.004295227236043034}. Best is trial 22 with value: 543.0484619140625.\n",
      "[I 2025-12-03 15:43:18,625] Trial 27 finished with value: 550.4054565429688 and parameters: {'n_estimators': 3253, 'learning_rate': 0.006788358590658656, 'max_depth': 5, 'subsample': 0.9362206665275151, 'colsample_bytree': 0.8567659803195019, 'reg_alpha': 1.013965626868667e-08, 'reg_lambda': 0.050766468896529866}. Best is trial 22 with value: 543.0484619140625.\n",
      "[I 2025-12-03 15:43:27,596] Trial 28 finished with value: 550.7140299479166 and parameters: {'n_estimators': 2768, 'learning_rate': 0.01126354977468873, 'max_depth': 6, 'subsample': 0.777815176664201, 'colsample_bytree': 0.5583616039769715, 'reg_alpha': 0.0001720957184701667, 'reg_lambda': 0.24672105312570417}. Best is trial 22 with value: 543.0484619140625.\n",
      "[I 2025-12-03 15:43:30,602] Trial 29 finished with value: 539.716552734375 and parameters: {'n_estimators': 1294, 'learning_rate': 0.006105221355150224, 'max_depth': 4, 'subsample': 0.962799237342616, 'colsample_bytree': 0.6399268636522191, 'reg_alpha': 1.5572008693758735e-05, 'reg_lambda': 0.00011976988267010149}. Best is trial 29 with value: 539.716552734375.\n",
      "[I 2025-12-03 15:43:34,382] Trial 30 finished with value: 540.8539428710938 and parameters: {'n_estimators': 1330, 'learning_rate': 0.005881923374335655, 'max_depth': 5, 'subsample': 0.966506326471976, 'colsample_bytree': 0.6279876758989109, 'reg_alpha': 1.3539663917778261e-05, 'reg_lambda': 1.949039891304474e-06}. Best is trial 29 with value: 539.716552734375.\n",
      "[I 2025-12-03 15:43:38,078] Trial 31 finished with value: 541.2431844075521 and parameters: {'n_estimators': 1264, 'learning_rate': 0.005713241940409027, 'max_depth': 5, 'subsample': 0.980266974536451, 'colsample_bytree': 0.6231725031706055, 'reg_alpha': 9.743085007279158e-06, 'reg_lambda': 8.208315551240598e-07}. Best is trial 29 with value: 539.716552734375.\n",
      "[I 2025-12-03 15:43:41,460] Trial 32 finished with value: 541.1870320638021 and parameters: {'n_estimators': 1207, 'learning_rate': 0.005688267526035358, 'max_depth': 5, 'subsample': 0.970845042705565, 'colsample_bytree': 0.6368595161437461, 'reg_alpha': 1.2210929932809243e-05, 'reg_lambda': 1.6286733079132604e-06}. Best is trial 29 with value: 539.716552734375.\n",
      "[I 2025-12-03 15:43:44,827] Trial 33 finished with value: 540.7225952148438 and parameters: {'n_estimators': 1250, 'learning_rate': 0.005540049456906711, 'max_depth': 5, 'subsample': 0.9648170784386539, 'colsample_bytree': 0.6299688126821846, 'reg_alpha': 1.3086292663734103e-05, 'reg_lambda': 9.370712133797551e-07}. Best is trial 29 with value: 539.716552734375.\n",
      "[I 2025-12-03 15:43:48,245] Trial 34 finished with value: 541.0758463541666 and parameters: {'n_estimators': 1211, 'learning_rate': 0.0060503185785241095, 'max_depth': 5, 'subsample': 0.9691336845506512, 'colsample_bytree': 0.6544121622552629, 'reg_alpha': 2.4249617814095917e-05, 'reg_lambda': 5.087998396635649e-07}. Best is trial 29 with value: 539.716552734375.\n",
      "[I 2025-12-03 15:43:52,675] Trial 35 finished with value: 540.9300944010416 and parameters: {'n_estimators': 1601, 'learning_rate': 0.005013458967580869, 'max_depth': 5, 'subsample': 0.9662596986995913, 'colsample_bytree': 0.6653993256994304, 'reg_alpha': 2.5754577754094588e-05, 'reg_lambda': 1.136992662918154e-08}. Best is trial 29 with value: 539.716552734375.\n",
      "[I 2025-12-03 15:43:58,193] Trial 36 finished with value: 543.3799438476562 and parameters: {'n_estimators': 1593, 'learning_rate': 0.0051047439950129255, 'max_depth': 6, 'subsample': 0.9991901659889061, 'colsample_bytree': 0.5464578851143272, 'reg_alpha': 0.00012228616909951007, 'reg_lambda': 1.0079520555043702e-08}. Best is trial 29 with value: 539.716552734375.\n",
      "[I 2025-12-03 15:44:01,155] Trial 37 finished with value: 539.2318115234375 and parameters: {'n_estimators': 1549, 'learning_rate': 0.007807089758192301, 'max_depth': 3, 'subsample': 0.899369671646619, 'colsample_bytree': 0.6729580260325667, 'reg_alpha': 0.0004567061916459353, 'reg_lambda': 4.21876791686148e-05}. Best is trial 37 with value: 539.2318115234375.\n",
      "[I 2025-12-03 15:44:04,113] Trial 38 finished with value: 539.0059204101562 and parameters: {'n_estimators': 1472, 'learning_rate': 0.007973044745707755, 'max_depth': 3, 'subsample': 0.8536924433424946, 'colsample_bytree': 0.603786608598915, 'reg_alpha': 0.002140861940880286, 'reg_lambda': 4.405405628260077e-05}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:07,652] Trial 39 finished with value: 542.2573649088541 and parameters: {'n_estimators': 1756, 'learning_rate': 0.01761658016195157, 'max_depth': 3, 'subsample': 0.8488953676778064, 'colsample_bytree': 0.5983701889009985, 'reg_alpha': 0.0009672822776980997, 'reg_lambda': 3.715355830152891e-05}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:10,678] Trial 40 finished with value: 539.7099812825521 and parameters: {'n_estimators': 1497, 'learning_rate': 0.008093348255510512, 'max_depth': 3, 'subsample': 0.8986291881532291, 'colsample_bytree': 0.7194156920956113, 'reg_alpha': 0.008454502087241626, 'reg_lambda': 7.780680831537115e-05}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:13,663] Trial 41 finished with value: 539.6039428710938 and parameters: {'n_estimators': 1490, 'learning_rate': 0.008052370402773507, 'max_depth': 3, 'subsample': 0.8956281667356473, 'colsample_bytree': 0.7038439008577348, 'reg_alpha': 0.033894644651629814, 'reg_lambda': 3.530737948964591e-05}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:16,655] Trial 42 finished with value: 540.0741780598959 and parameters: {'n_estimators': 1507, 'learning_rate': 0.010050841166618528, 'max_depth': 3, 'subsample': 0.8955880307643651, 'colsample_bytree': 0.6975239470966473, 'reg_alpha': 0.006959924843596642, 'reg_lambda': 7.928784428240496e-05}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:20,461] Trial 43 finished with value: 540.294189453125 and parameters: {'n_estimators': 1996, 'learning_rate': 0.007922241461390356, 'max_depth': 3, 'subsample': 0.7083900033014822, 'colsample_bytree': 0.7151739903956966, 'reg_alpha': 0.004756126098723497, 'reg_lambda': 9.889033567740804e-06}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:23,915] Trial 44 finished with value: 541.5681762695312 and parameters: {'n_estimators': 1788, 'learning_rate': 0.013571616003538366, 'max_depth': 3, 'subsample': 0.8575254511637402, 'colsample_bytree': 0.6807190121948232, 'reg_alpha': 0.07072617030197047, 'reg_lambda': 0.0001595210518356162}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:26,160] Trial 45 finished with value: 539.1984456380209 and parameters: {'n_estimators': 1126, 'learning_rate': 0.010032193627887634, 'max_depth': 3, 'subsample': 0.8973536618384779, 'colsample_bytree': 0.7322020014185402, 'reg_alpha': 0.043019229725129164, 'reg_lambda': 2.5200188624614896e-05}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:28,325] Trial 46 finished with value: 539.5223185221354 and parameters: {'n_estimators': 1043, 'learning_rate': 0.010243432708142215, 'max_depth': 3, 'subsample': 0.8980455208617814, 'colsample_bytree': 0.7519956321143019, 'reg_alpha': 0.0395997080585407, 'reg_lambda': 1.3304528305945071e-05}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:30,275] Trial 47 finished with value: 539.3850911458334 and parameters: {'n_estimators': 1014, 'learning_rate': 0.010437376531512456, 'max_depth': 3, 'subsample': 0.7863433103284074, 'colsample_bytree': 0.7266539340749434, 'reg_alpha': 0.0707015180177367, 'reg_lambda': 1.1295151268102258e-05}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:33,810] Trial 48 finished with value: 540.0928955078125 and parameters: {'n_estimators': 1035, 'learning_rate': 0.015651631913221054, 'max_depth': 3, 'subsample': 0.7844575678507925, 'colsample_bytree': 0.7416554629186481, 'reg_alpha': 1.2981622562912836, 'reg_lambda': 1.1800932067814935e-05}. Best is trial 38 with value: 539.0059204101562.\n",
      "[I 2025-12-03 15:44:37,812] Trial 49 finished with value: 540.0607299804688 and parameters: {'n_estimators': 1000, 'learning_rate': 0.010289228637095141, 'max_depth': 3, 'subsample': 0.7176679512544089, 'colsample_bytree': 0.7586240972026025, 'reg_alpha': 0.1821259935105514, 'reg_lambda': 6.802054955056302e-06}. Best is trial 38 with value: 539.0059204101562.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ¡MEJORES PARÁMETROS ENCONTRADOS! ---\n",
      "{'n_estimators': 1472, 'learning_rate': 0.007973044745707755, 'max_depth': 3, 'subsample': 0.8536924433424946, 'colsample_bytree': 0.603786608598915, 'reg_alpha': 0.002140861940880286, 'reg_lambda': 4.405405628260077e-05}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Función Objetivo para Optuna\n",
    "def objective(trial):\n",
    "    # 1. Sugerir Hiperparámetros (El \"Espacio de Búsqueda\")\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 4000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'objective': 'reg:squarederror'\n",
    "    }\n",
    "\n",
    "    # 2. Crear modelo con esos parámetros\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "\n",
    "    # 3. Evaluar con Validación Cruzada (Robustez)\n",
    "    # Usamos RMSE negativo porque cross_val_score trata de maximizar\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train_ready, y_train, cv=cv, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "    # 4. Devolver el error promedio (Optuna intentará minimizar esto si se lo decimos)\n",
    "    return -scores.mean()\n",
    "\n",
    "# Ejecutar la búsqueda\n",
    "print(\"Iniciando optimización de XGBoost con Optuna...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50) # 30 pruebas (puedes subir a 50 o 100 si tienes tiempo)\n",
    "\n",
    "print(\"\\n--- ¡MEJORES PARÁMETROS ENCONTRADOS! ---\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. DEFINIENDO LOS JUGADORES (BASE LEARNERS) ---\n",
      "--- 2. ENTRENANDO EL STACKING REGRESSOR ---\n",
      "¡Entrenamiento completado!\n",
      "\n",
      "--- 3. EVALUACIÓN ---\n",
      "RMSE Stacking Final: 536.18\n",
      "RMSE Promedio Simple (Ref): 535.49\n"
     ]
    }
   ],
   "source": [
    "# [NUEVA CELDA DE MODELADO AVANZADO - STACKING]\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- 1. DEFINIENDO LOS JUGADORES (BASE LEARNERS) ---\")\n",
    "\n",
    "# XGBoost: Configuracion robusta (sin early_stopping manual para evitar conflictos en Stacking)\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    n_estimators=1472,\n",
    "    learning_rate=0.007973044745707755,      # Aprendizaje lento y preciso\n",
    "    max_depth=3,\n",
    "    subsample=0.8536924433424946,\n",
    "    colsample_bytree=0.603786608598915,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# LightGBM: El velocista\n",
    "lgb_reg = lgb.LGBMRegressor(\n",
    "    n_estimators=1472,\n",
    "    learning_rate=0.007973044745707755,\n",
    "    num_leaves=31,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_fraction=0.7,\n",
    "    bagging_freq=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# CatBoost: El nuevo fichaje (Maneja genial las interacciones no lineales)\n",
    "cat_reg = CatBoostRegressor(\n",
    "    iterations=1472,\n",
    "    learning_rate=0.007973044745707755,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,           # Regularización L2 fuerte\n",
    "    verbose=0,               # Silencioso\n",
    "    random_state=42,\n",
    "    allow_writing_files=False # Evita crear archivos de log basura\n",
    ")\n",
    "\n",
    "# Lista de modelos para el Stacking\n",
    "estimators = [\n",
    "    ('xgb', xgb_reg),\n",
    "    ('lgb', lgb_reg),\n",
    "    ('cat', cat_reg)\n",
    "]\n",
    "\n",
    "print(\"--- 2. ENTRENANDO EL STACKING REGRESSOR ---\")\n",
    "# El StackingRegressor hace la validación cruzada interna (cv=5) automáticamente\n",
    "# para generar las predicciones que usará el meta-modelo.\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RidgeCV(), # Meta-modelo lineal con auto-tuning de regularización\n",
    "    cv=5,                      # 5-Fold CV interno (Clave para la robustez)\n",
    "    n_jobs=-1,\n",
    "    passthrough=False          # El meta-modelo solo ve las predicciones, no los datos originales\n",
    ")\n",
    "\n",
    "# Entrenamos en tus datos de TRAIN procesados\n",
    "stacking_model.fit(X_train_ready, y_train)\n",
    "print(\"¡Entrenamiento completado!\")\n",
    "\n",
    "print(\"\\n--- 3. EVALUACIÓN ---\")\n",
    "# Evaluamos en tu set de validación (que el stacking no ha visto durante el fit)\n",
    "val_preds = stacking_model.predict(X_val_ready)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "\n",
    "print(f\"RMSE Stacking Final: {rmse_val:.2f}\")\n",
    "\n",
    "# Comparativa rápida con el promedio simple para ver la ganancia\n",
    "# (Solo informativo, el modelo bueno es el stacking)\n",
    "preds_xgb = stacking_model.estimators_[0].predict(X_val_ready)\n",
    "preds_lgb = stacking_model.estimators_[1].predict(X_val_ready)\n",
    "preds_cat = stacking_model.estimators_[2].predict(X_val_ready)\n",
    "rmse_avg = np.sqrt(mean_squared_error(y_val, (preds_xgb+preds_lgb+preds_cat)/3))\n",
    "print(f\"RMSE Promedio Simple (Ref): {rmse_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_preds_stacking = stacking_model.predict(test_kaggle_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your predictions in a `submission.csv` file for scoring on the [leaderboard](https://www.kaggle.com/competitions/u-tad-birth-weight-point-prediction-2025/leaderboard)\n",
    "To submit your notebook click on **Submit to competition** and then **Submit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T16:14:19.922232Z",
     "iopub.status.busy": "2025-10-06T16:14:19.921888Z",
     "iopub.status.idle": "2025-10-06T16:14:19.936268Z",
     "shell.execute_reply": "2025-10-06T16:14:19.935662Z",
     "shell.execute_reply.started": "2025-10-06T16:14:19.922198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# do not modify this code\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission[\"DBWT\"] = kaggle_preds_stacking\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13892590,
     "sourceId": 116374,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

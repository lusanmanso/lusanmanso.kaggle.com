{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1052f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Carga y Enriquecimiento de Datos (Focus: Extremos) ---\n",
      "--- 2. Configurando Stacking 'Agresivo' ---\n",
      "--- 3. Entrenando Stacking Final ---\n",
      "--- 4. Post-Procesamiento: Calibración de Varianza ---\n",
      "   Original Pred Std: 367.98 | Target Train Std: 579.88\n",
      "   Predicciones re-escaladas (Factor: 1.5758)\n",
      "¡Hecho! Generado 'final_calibrated_submission.csv'\n",
      "\n",
      "--- Estadísticas Finales ---\n",
      "Min: 505.73\n",
      "Max: 4735.06\n",
      "Std: 579.88\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "\n",
    "def load_and_enrich_data(train_path, test_path):\n",
    "    print(\"--- 1. Carga y Enriquecimiento de Datos (Focus: Extremos) ---\")\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    target = 'DBWT'\n",
    "\n",
    "    train['is_train'] = 1\n",
    "    test['is_train'] = 0\n",
    "\n",
    "    # Alinear columnas\n",
    "    missing_cols = set(train.columns) - set(test.columns) - {target}\n",
    "    for c in missing_cols:\n",
    "        test[c] = 0\n",
    "\n",
    "    df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "    # --- A. GESTIÓN DE EXTREMOS (UserGuide 2018) ---\n",
    "\n",
    "    # 1. PLURALIDAD (DPLURAL): Clave para bajo peso (Gemelos/Trillizos)\n",
    "    # Si existe, la limpiamos. Si no, intentamos inferirla o ignoramos.\n",
    "    if 'DPLURAL' in df.columns:\n",
    "        # User Guide: 1=Single, 2=Twin, 3=Triplet, etc.\n",
    "        # Creamos flag 'MULTIPLE_BIRTH' porque la diferencia masiva es 1 vs Resto\n",
    "        df['MULTIPLE_BIRTH'] = df['DPLURAL'].apply(lambda x: 1 if x > 1 else 0)\n",
    "\n",
    "    # 2. GESTACIÓN (GESTREC3 / COMBGEST): Clave para prematuros\n",
    "    if 'GESTREC3' in df.columns:\n",
    "        # 1=Under 37 weeks (Prematuro), 2=37+, 3=Unknown\n",
    "        # Ojo con el 3 (Unknown), mejor tratarlo como categoría aparte\n",
    "        df['IS_PREMATURE'] = df['GESTREC3'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "    # 3. SEXO: Pequeña pero constante diferencia\n",
    "    if 'SEX' in df.columns:\n",
    "        df['IS_MALE'] = df['SEX'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "    # --- B. LIMPIEZA STANDARD ---\n",
    "    cig_cols = ['CIG_0', 'CIG_1', 'CIG_2', 'CIG_3']\n",
    "    for col in cig_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace([99, 999], np.nan)\n",
    "\n",
    "    if set(cig_cols).issubset(df.columns):\n",
    "        df['TOTAL_CIGS'] = df[cig_cols].sum(axis=1, min_count=1)\n",
    "\n",
    "    # RF (Risk Factors) a conteo numérico\n",
    "    rf_cols = [c for c in df.columns if c.startswith('RF_')]\n",
    "    if rf_cols:\n",
    "        for col in rf_cols:\n",
    "            df[col] = df[col].apply(lambda x: 1 if str(x).upper() in ['Y', '1', 'YES'] else 0)\n",
    "        df['RISK_SCORE'] = df[rf_cols].sum(axis=1)\n",
    "\n",
    "    # Imputación\n",
    "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    df[cat_cols] = df[cat_cols].fillna('Missing')\n",
    "\n",
    "    return df, target\n",
    "\n",
    "def get_aggressive_stacking():\n",
    "    \"\"\"\n",
    "    Stacking configurado para ser menos conservador (menos regularización).\n",
    "    \"\"\"\n",
    "    print(\"--- 2. Configurando Stacking 'Agresivo' ---\")\n",
    "\n",
    "    # Preprocesador\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', RobustScaler(), lambda X: X.select_dtypes(include=['int64', 'float64']).columns),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "             lambda X: X.select_dtypes(include=['object', 'category']).columns)\n",
    "        ])\n",
    "\n",
    "    # Modelos Base: Reducimos regularización para permitir capturar colas\n",
    "    # XGBoost: bajamos min_child_weight y reg_lambda\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.03, # Más lento pero más preciso\n",
    "        max_depth=7,        # Más profundidad = más complejidad\n",
    "        min_child_weight=1, # Permite hojas con pocos datos (extremos)\n",
    "        reg_lambda=1,       # Regularización estándar (antes era más alta implícitamente)\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    lgbm = LGBMRegressor(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=40,      # Más hojas\n",
    "        min_child_samples=10, # Permitir grupos pequeños\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    cat = CatBoostRegressor(\n",
    "        iterations=1500,\n",
    "        learning_rate=0.03,\n",
    "        depth=7,\n",
    "        l2_leaf_reg=1, # Menor regularización L2\n",
    "        verbose=0,\n",
    "        random_seed=SEED,\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "\n",
    "    # Meta Learner: Ridge con alpha bajo para confiar más en los base learners\n",
    "    meta_learner = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "\n",
    "    stacking = StackingRegressor(\n",
    "        estimators=[('xgb', xgb), ('lgbm', lgbm), ('cat', cat)],\n",
    "        final_estimator=meta_learner,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    final_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', stacking)\n",
    "    ])\n",
    "\n",
    "    # Mantenemos log1p porque es matemáticamente sólido para pesos\n",
    "    return TransformedTargetRegressor(regressor=final_pipeline, func=np.log1p, inverse_func=np.expm1)\n",
    "\n",
    "def post_process_variance(y_pred, y_train_sample):\n",
    "    \"\"\"\n",
    "    Técnica de 'Variance Inflation' o 'Histogram Matching' simple.\n",
    "    Ajusta la distribución de predicciones para igualar la desviación típica del train.\n",
    "\n",
    "    Fórmula: y_new = (y - mu_pred) * (sigma_train / sigma_pred) + mu_train\n",
    "    \"\"\"\n",
    "    print(\"--- 4. Post-Procesamiento: Calibración de Varianza ---\")\n",
    "\n",
    "    mu_train = np.mean(y_train_sample)\n",
    "    sigma_train = np.std(y_train_sample)\n",
    "\n",
    "    mu_pred = np.mean(y_pred)\n",
    "    sigma_pred = np.std(y_pred)\n",
    "\n",
    "    print(f\"   Original Pred Std: {sigma_pred:.2f} | Target Train Std: {sigma_train:.2f}\")\n",
    "\n",
    "    # Factor de escala (Limitado para no romper nada, ej. max 1.5x)\n",
    "    scale = sigma_train / sigma_pred\n",
    "\n",
    "    # Aplicamos corrección suave (mezcla entre original y estirado para seguridad)\n",
    "    # y_new = (y_pred - mu_pred) * scale + mu_train\n",
    "    # A menudo basta con escalar la desviación alrededor de la propia media predicha\n",
    "    # si confiamos en el bias del modelo. Usaremos la media del train para corregir bias también.\n",
    "\n",
    "    y_calibrated = (y_pred - mu_pred) * scale + mu_train\n",
    "\n",
    "    print(f\"   Predicciones re-escaladas (Factor: {scale:.4f})\")\n",
    "\n",
    "    # Safety Check: No bajar de 0\n",
    "    y_calibrated = np.maximum(y_calibrated, 100)\n",
    "\n",
    "    return y_calibrated\n",
    "\n",
    "def main():\n",
    "    TRAIN_FILE = 'train.csv'\n",
    "    TEST_FILE = 'test.csv'\n",
    "\n",
    "    try:\n",
    "        df, target_col = load_and_enrich_data(TRAIN_FILE, TEST_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Faltan archivos.\")\n",
    "        return\n",
    "\n",
    "    train_df = df[df['is_train'] == 1].drop(['is_train'], axis=1)\n",
    "    test_df = df[df['is_train'] == 0].drop(['is_train', target_col], axis=1)\n",
    "\n",
    "    X = train_df.drop(target_col, axis=1)\n",
    "    y = train_df[target_col]\n",
    "\n",
    "    # Entrenar\n",
    "    model = get_aggressive_stacking()\n",
    "    print(\"--- 3. Entrenando Stacking Final ---\")\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predecir\n",
    "    raw_preds = model.predict(test_df)\n",
    "\n",
    "    # --- PASO CRÍTICO: CALIBRACIÓN ---\n",
    "    final_preds = post_process_variance(raw_preds, y)\n",
    "\n",
    "    # Guardar\n",
    "    submission = pd.DataFrame({'id': test_df.index, 'DBWT': final_preds})\n",
    "    submission.to_csv('final_calibrated_submission.csv', index=False)\n",
    "    print(\"¡Hecho! Generado 'final_calibrated_submission.csv'\")\n",
    "\n",
    "    # Mini-reporte\n",
    "    print(\"\\n--- Estadísticas Finales ---\")\n",
    "    print(f\"Min: {final_preds.min():.2f}\")\n",
    "    print(f\"Max: {final_preds.max():.2f}\")\n",
    "    print(f\"Std: {np.std(final_preds):.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69b1407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Carga y Enriquecimiento (Fase Semi-Supervisada) ---\n",
      "--- STAGE 1: Entrenamiento Inicial ---\n",
      "--- Generando Pseudo-Etiquetas para el Test Set ---\n",
      "--- STAGE 2: Re-Entrenamiento con Pseudo-Labeling ---\n",
      "¡Generado 'pseudo_labeling_submission.csv'!\n",
      "Stats Finales -> Min: 437.70, Max: 4993.67, Std: 579.88\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "\n",
    "# --- REUTILIZAMOS LA LÓGICA DE CARGA ---\n",
    "def load_and_enrich_data(train_path, test_path):\n",
    "    print(\"--- 1. Carga y Enriquecimiento (Fase Semi-Supervisada) ---\")\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    target = 'DBWT'\n",
    "\n",
    "    train['is_train'] = 1\n",
    "    test['is_train'] = 0\n",
    "\n",
    "    # Alinear columnas\n",
    "    missing_cols = set(train.columns) - set(test.columns) - {target}\n",
    "    for c in missing_cols:\n",
    "        test[c] = 0\n",
    "\n",
    "    df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "    # Lógica de Extremos\n",
    "    if 'DPLURAL' in df.columns:\n",
    "        df['MULTIPLE_BIRTH'] = df['DPLURAL'].apply(lambda x: 1 if x > 1 else 0)\n",
    "\n",
    "    if 'GESTREC3' in df.columns:\n",
    "        df['IS_PREMATURE'] = df['GESTREC3'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "    if 'SEX' in df.columns:\n",
    "        df['IS_MALE'] = df['SEX'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "    # Limpieza\n",
    "    cig_cols = ['CIG_0', 'CIG_1', 'CIG_2', 'CIG_3']\n",
    "    for col in cig_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace([99, 999], np.nan)\n",
    "\n",
    "    if set(cig_cols).issubset(df.columns):\n",
    "        df['TOTAL_CIGS'] = df[cig_cols].sum(axis=1, min_count=1)\n",
    "\n",
    "    rf_cols = [c for c in df.columns if c.startswith('RF_')]\n",
    "    if rf_cols:\n",
    "        for col in rf_cols:\n",
    "            df[col] = df[col].apply(lambda x: 1 if str(x).upper() in ['Y', '1', 'YES'] else 0)\n",
    "        df['RISK_SCORE'] = df[rf_cols].sum(axis=1)\n",
    "\n",
    "    # Imputación\n",
    "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    df[cat_cols] = df[cat_cols].fillna('Missing')\n",
    "\n",
    "    return df, target\n",
    "\n",
    "def get_model():\n",
    "    # Misma configuración agresiva que funcionó\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', RobustScaler(), lambda X: X.select_dtypes(include=['int64', 'float64']).columns),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "             lambda X: X.select_dtypes(include=['object', 'category']).columns)\n",
    "        ])\n",
    "\n",
    "    xgb = XGBRegressor(n_estimators=1500, learning_rate=0.03, max_depth=7, min_child_weight=1, reg_lambda=1, n_jobs=-1, random_state=SEED)\n",
    "    lgbm = LGBMRegressor(n_estimators=1500, learning_rate=0.03, num_leaves=40, min_child_samples=10, n_jobs=-1, random_state=SEED, verbose=-1)\n",
    "    cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=7, l2_leaf_reg=1, verbose=0, random_seed=SEED, allow_writing_files=False)\n",
    "    meta = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "\n",
    "    stacking = StackingRegressor(\n",
    "        estimators=[('xgb', xgb), ('lgbm', lgbm), ('cat', cat)],\n",
    "        final_estimator=meta,\n",
    "        cv=5, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return TransformedTargetRegressor(regressor=Pipeline([('preprocessor', preprocessor), ('model', stacking)]), func=np.log1p, inverse_func=np.expm1)\n",
    "\n",
    "def main():\n",
    "    TRAIN_FILE = 'train.csv'\n",
    "    TEST_FILE = 'test.csv'\n",
    "\n",
    "    # 1. CARGA INICIAL\n",
    "    try:\n",
    "        df, target_col = load_and_enrich_data(TRAIN_FILE, TEST_FILE)\n",
    "    except FileNotFoundError: return\n",
    "\n",
    "    train_df = df[df['is_train'] == 1].drop(['is_train'], axis=1)\n",
    "    test_df = df[df['is_train'] == 0].drop(['is_train', target_col], axis=1)\n",
    "\n",
    "    X = train_df.drop(target_col, axis=1)\n",
    "    y = train_df[target_col]\n",
    "\n",
    "    # 2. ENTRENAMIENTO INICIAL (STAGE 1)\n",
    "    print(\"--- STAGE 1: Entrenamiento Inicial ---\")\n",
    "    model = get_model()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # 3. GENERAR PSEUDO-LABELS\n",
    "    print(\"--- Generando Pseudo-Etiquetas para el Test Set ---\")\n",
    "    pseudo_preds = model.predict(test_df)\n",
    "\n",
    "    # Post-proceso inicial para asegurar calidad de pseudo-labels\n",
    "    # (Usamos la media/std del train para calibrar las pseudo-labels antes de reentrenar)\n",
    "    mu_train, sigma_train = np.mean(y), np.std(y)\n",
    "    mu_pred, sigma_pred = np.mean(pseudo_preds), np.std(pseudo_preds)\n",
    "    pseudo_preds_calibrated = (pseudo_preds - mu_pred) * (sigma_train / sigma_pred) + mu_train\n",
    "\n",
    "    # 4. CREAR DATASET AUMENTADO (TRAIN + PSEUDO_TEST)\n",
    "    print(\"--- STAGE 2: Re-Entrenamiento con Pseudo-Labeling ---\")\n",
    "    X_test = test_df.copy()\n",
    "    X_test[target_col] = pseudo_preds_calibrated # Asignamos la predicción como si fuera real\n",
    "\n",
    "    # Concatenamos Train real con Test \"falso\"\n",
    "    # Unimos X e y de train\n",
    "    train_full = X.copy()\n",
    "    train_full[target_col] = y\n",
    "\n",
    "    # Dataset gigante\n",
    "    augmented_train = pd.concat([train_full, X_test], axis=0)\n",
    "\n",
    "    X_aug = augmented_train.drop(target_col, axis=1)\n",
    "    y_aug = augmented_train[target_col]\n",
    "\n",
    "    # 5. ENTRENAMIENTO FINAL (STAGE 2)\n",
    "    final_model = get_model() # Instancia nueva limpia\n",
    "    final_model.fit(X_aug, y_aug)\n",
    "\n",
    "    # 6. PREDICCIÓN FINAL\n",
    "    final_preds = final_model.predict(test_df)\n",
    "\n",
    "    # Calibración Final (Variance Inflation sobre el resultado final)\n",
    "    # Usamos la std original del train real, no del aumentado\n",
    "    mu_fin, sigma_fin = np.mean(final_preds), np.std(final_preds)\n",
    "    final_preds_calibrated = (final_preds - mu_fin) * (sigma_train / sigma_fin) + mu_train\n",
    "    final_preds_calibrated = np.maximum(final_preds_calibrated, 100)\n",
    "\n",
    "    submission = pd.DataFrame({'id': test_df.index, 'DBWT': final_preds_calibrated})\n",
    "    submission.to_csv('pseudo_labeling_submission.csv', index=False)\n",
    "    print(\"¡Generado 'pseudo_labeling_submission.csv'!\")\n",
    "    print(f\"Stats Finales -> Min: {final_preds_calibrated.min():.2f}, Max: {final_preds_calibrated.max():.2f}, Std: {np.std(final_preds_calibrated):.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
